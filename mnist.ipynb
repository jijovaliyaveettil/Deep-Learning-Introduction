{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of zipfile & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some facts about the dataset:\n",
    "- The dataset is a collection of handwritten digits (0-9) from the MNIST database.\n",
    "- Each image is a 28x28 pixel grayscale image.\n",
    "- 42000 rows indicate the number of images\n",
    "- 784 columns indicate the number of pixels in each image\n",
    "- 0-9 labels indicate the digit represented by the image\n",
    "- the value of each pixel is between 0-255 where 0 is black and 255 is white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [7, 0, 0, ..., 0, 0, 0],\n",
       "       [6, 0, 0, ..., 0, 0, 0],\n",
       "       [9, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the DataFrame to a NumPy array\n",
    "data = np.array(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 7, 6, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = data[50,1:].reshape(28, 28)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data[:, 0].astype(int)  \n",
    "pixels = data[:, 1:]\n",
    "pixels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization:\n",
    "\n",
    "Scale pixel values from [0, 255] to [0, 1]\n",
    "\n",
    "Why?: Prevents large input values from destabilizing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels = pixels / 255.0\n",
    "pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding:\n",
    "\n",
    "Convert labels to 10-dimensional vectors (e.g., label 3 â†’ [0,0,0,1,0,0,0,0,0,0])\n",
    "\n",
    "Why?: Matches the output layer dimension for softmax classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(np.unique(labels))\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can write a onehot function \n",
    "one_hot_labels = np.eye(num_labels)[labels]  # Shape: (42000, 10)\n",
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 784)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels[:40000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we randomise the dataset for better results ?\n",
    "X_train, X_test = pixels[:40000], pixels[40000:]\n",
    "Y_train, Y_test = one_hot_labels[:40000], one_hot_labels[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 784) (40000, 10)\n",
      "(2000, 784) (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the training and test sets\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of weights and biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94027361 0.95005829]\n",
      " [0.28556778 0.9849777 ]\n",
      " [0.39436223 0.43411518]]\n",
      "[[ 2.20163852  1.36614437]\n",
      " [-0.85390301  0.8905027 ]\n",
      " [ 0.10171543  0.60226587]]\n"
     ]
    }
   ],
   "source": [
    "# Generate random numbers\n",
    "uniform_random_numbers = np.random.rand(3, 2)\n",
    "normal_random_numbers = np.random.randn(3, 2)\n",
    "print(uniform_random_numbers)\n",
    "print(normal_random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original random numbers:\n",
      " [[-0.65656939 -0.19794362]\n",
      " [-1.54123519 -0.05240382]\n",
      " [ 0.38850029  0.60545812]]\n",
      "Scaled random numbers:\n",
      " [[-0.29362676 -0.08852308]\n",
      " [-0.68926133 -0.0234357 ]\n",
      " [ 0.17374261  0.2707691 ]]\n"
     ]
    }
   ],
   "source": [
    "n = 10  # Example value for n\n",
    "scaling_factor = np.sqrt(2 / n)\n",
    "\n",
    "# Generate a 3x2 array of random numbers from a standard normal distribution\n",
    "normal_random_numbers = np.random.randn(3, 2)\n",
    "\n",
    "# Scale the random numbers\n",
    "scaled_random_numbers = normal_random_numbers * scaling_factor\n",
    "\n",
    "print(\"Original random numbers:\\n\", normal_random_numbers)\n",
    "print(\"Scaled random numbers:\\n\", scaled_random_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reasons for the chosen initialization:\n",
    "- **Xavier Initialization**: Suitable for tanh activation functions. It keeps the variance of activations and gradients the same across layers.\n",
    "- **He Initialization**: Suitable for ReLU activation functions. It helps in maintaining the variance of activations and gradients across layers.\n",
    "- **Xavier Initialization**: Variance of weights is set to 1/n, where n is the number of input neurons.\n",
    "- **He Initialization**: Variance of weights is set to 2/n, where n is the number of input neurons.\n",
    "\n",
    "ReLU sets half the neurons to zero during forward propagation. He initialization compensates for this by doubling the variance of the weights, ensuring the output variance remains stable (prevents gradients from vanishing or exploding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reasons for the selection of 128 neurons:\n",
    "- **Empirical Evidence**: 128 neurons have been found to work well in practice for many tasks.\n",
    "- **Computational Efficiency**: 128 neurons are computationally efficient for training and inference.\n",
    "- **Flexibility**: 128 neurons provide a good balance between model complexity and generalization.\n",
    "- **Rule of Thumb**: Start with a hidden layer size between the input (784) and output (10) layers.\n",
    "\n",
    "Why the dimension of 784,128?\n",
    "\n",
    "For a layer with n_input neurons and n_output neurons, weights must have shape (n_input, n_output) as per matrix multiplication rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    np.random.seed(42)  # Fix randomness for reproducibility\n",
    "    \n",
    "    W1 = np.random.randn(784, 128) * np.sqrt(2 / 784)  # He initialization for ReLU\n",
    "    b1 = np.zeros((1, 128))                             # Zero bias\n",
    "    W2 = np.random.randn(128, 10) * np.sqrt(2 / 128)    # He initialization\n",
    "    b2 = np.zeros((1, 10))                              # Zero bias\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ReLU(Z):\n",
    "#     return np.maximum(Z, 0)\n",
    "\n",
    "# def softmax(Z):\n",
    "#     A = np.exp(Z) / sum(np.exp(Z))\n",
    "#     return A\n",
    "    \n",
    "# def forward_prop(W1, b1, W2, b2, X):\n",
    "#     Z1 = W1.dot(X) + b1\n",
    "#     A1 = ReLU(Z1)\n",
    "#     Z2 = W2.dot(A1) + b2\n",
    "#     A2 = softmax(Z2)\n",
    "#     return Z1, A1, Z2, A2\n",
    "\n",
    "# def ReLU_deriv(Z):\n",
    "#     return Z > 0\n",
    "\n",
    "# def one_hot(Y):\n",
    "#     one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "#     one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "#     one_hot_Y = one_hot_Y.T\n",
    "#     return one_hot_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why z - np.max ?\n",
    "\n",
    " If values in Z are large (e.g., 1000), np.exp(Z) becomes inf (overflow). therefore we Subtract the maximum value in Z before exponentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1    # Shape: (batch_size, 128)\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2   # Shape: (batch_size, 10)\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y_pred, Y_true):\n",
    "    return -np.mean(np.sum(Y_true * np.log(Y_pred + 1e-15), axis=1))  # +1e-15 to avoid log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "#     one_hot_Y = one_hot(Y)\n",
    "#     dZ2 = A2 - one_hot_Y\n",
    "#     dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "#     db2 = 1 / m * np.sum(dZ2)\n",
    "#     dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "#     dW1 = 1 / m * dZ1.dot(X.T)\n",
    "#     db1 = 1 / m * np.sum(dZ1)\n",
    "#     return dW1, db1, dW2, db2\n",
    "\n",
    "# def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "#     W1 = W1 - alpha * dW1\n",
    "#     b1 = b1 - alpha * db1    \n",
    "#     W2 = W2 - alpha * dW2  \n",
    "#     b2 = b2 - alpha * db2    \n",
    "#     return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, Y, Z1, A1, A2, W2):\n",
    "    batch_size = X.shape[0]\n",
    "    \n",
    "    # Output layer gradients\n",
    "    dZ2 = A2 - Y                            # Shape: (batch_size, 10)\n",
    "    dW2 = (A1.T @ dZ2) / batch_size          # Shape: (128, 10)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / batch_size\n",
    "    \n",
    "    # Hidden layer gradients\n",
    "    dZ1 = (dZ2 @ W2.T) * (Z1 > 0)           # Shape: (batch_size, 128)\n",
    "    dW1 = (X.T @ dZ1) / batch_size           # Shape: (784, 128)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / batch_size\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "#     W1 = W1 - alpha * dW1\n",
    "#     b1 = b1 - alpha * db1    \n",
    "#     W2 = W2 - alpha * dW2  \n",
    "#     b2 = b2 - alpha * db2    \n",
    "#     return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, lr=0.1):\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_predictions(A2):\n",
    "#     return np.argmax(A2, 0)\n",
    "\n",
    "# def get_accuracy(predictions, Y):\n",
    "#     print(predictions, Y)\n",
    "#     return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "# def gradient_descent(X, Y, alpha, iterations):\n",
    "#     W1, b1, W2, b2 = init_params()\n",
    "#     for i in range(iterations):\n",
    "#         Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "#         dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "#         W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "#         if i % 10 == 0:\n",
    "#             print(\"Iteration: \", i)\n",
    "#             predictions = get_predictions(A2)\n",
    "#             print(get_accuracy(predictions, Y))\n",
    "#     return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, epochs=100, batch_size=64, lr=0.1):\n",
    "    W1, b1, W2, b2 = initialize_parameters()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        permutation = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled = X_train[permutation]\n",
    "        Y_shuffled = Y_train[permutation]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            Y_batch = Y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            Z1, A1, Z2, A2 = forward(X_batch, W1, b1, W2, b2)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2 = backward(X_batch, Y_batch, Z1, A1, A2, W2)\n",
    "            \n",
    "            # Update parameters\n",
    "            W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, lr)\n",
    "        \n",
    "        # Print loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            _, _, _, A2 = forward(X_train, W1, b1, W2, b2)\n",
    "            loss = compute_loss(A2, Y_train)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2615\n",
      "Epoch 10, Loss: 0.0562\n",
      "Epoch 20, Loss: 0.0251\n",
      "Epoch 30, Loss: 0.0120\n",
      "Epoch 40, Loss: 0.0075\n",
      "Epoch 50, Loss: 0.0049\n",
      "Epoch 60, Loss: 0.0035\n",
      "Epoch 70, Loss: 0.0027\n",
      "Epoch 80, Loss: 0.0022\n",
      "Epoch 90, Loss: 0.0018\n",
      "Test Accuracy: 97.20%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(X, Y, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward(X, W1, b1, W2, b2)\n",
    "    predictions = np.argmax(A2, axis=1)\n",
    "    true_labels = np.argmax(Y, axis=1)\n",
    "    return np.mean(predictions == true_labels)\n",
    "\n",
    "# Train the model\n",
    "W1, b1, W2, b2 = train(X_train, Y_train)\n",
    "\n",
    "# Test accuracy\n",
    "test_acc = accuracy(X_test, Y_test, W1, b1, W2, b2)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
